<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Project 5: Fun With Diffusion</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <style>
body {
  background-color: #f8f8f8;
  font-family: 'Inter', sans-serif;
  color: #222;
  margin: 0;
  padding: 0;
  line-height: 1.6;
}

.container {
  max-width: 1200px;
  margin: 0 auto;
  padding: 60px 20px;
  background-color: white;
  box-shadow: 0 0 15px rgba(0,0,0,0.05);
  border-radius: 12px;
}

h1 {
  text-align: center;
  margin-bottom: 40px;
}

h2 {
  border-bottom: 1px solid #ddd;
  padding-bottom: 8px;
  margin-top: 40px;
}

img {
  max-width: 90%;
  height: auto;
  border-radius: 6px;
  transition: transform 0.3s ease, box-shadow 0.3s ease;
  cursor: zoom-in;
}

img:hover {
  transform: scale(1.03);
  box-shadow: 0 4px 20px rgba(0,0,0,0.1);
}

figure {
  text-align: center;
  margin: 20px 0;
}

figcaption {
  font-size: 0.9em;
  color: #666;
  margin-top: 6px;
}

/* Lightbox overlay */
.lightbox {
  display: none;
  position: fixed;
  z-index: 999;
  top: 0; left: 0; right: 0; bottom: 0;
  background: rgba(0,0,0,0.8);
  justify-content: center;
  align-items: center;
}

.lightbox img {
  max-width: 100%;
  max-height: 100%;
  border-radius: 8px;
  cursor: zoom-out;
}

.lightbox.active {
  display: flex;
}
</style>
</head>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			* {
				box-sizing: border-box;
			}
			h1 {
				text-align: center;
			}

			.container {
				display: flex;
				flex-direction: column;
				align-items: center;
				justify-content: center;
			}


			body {
				font-family: 'Inter', sans-serif;
			    margin: auto;
				width: 80%; 
			}
			.column {
				float: left;
				width: 50%;
				padding: 5px;
			}
			.column2 {
				float: left;
				width: 33.33%;
				padding: 5px;
			}
      		.column3 {
				float: left;
				width: 25%;
				padding: 5px;
			}
      		.column4 {
				float: left;
				width: 100%;
				padding: 5px;        
      		}
      		.column5 {
				float: left;
				width: 20%;
				padding: 5px;        
      		}
     		.column6 {
				float: left;
				width: 14.25%;
				padding: 5px;        
      		}
     		.column7 {
				float: left;
				width: 100%;
				padding: 5px;        
      		}
			.row::after {
			content: "";
			clear: both;
			display: table;
			}

		</style>
	</head>
	<body>
		<div class="container">
			<h1>Project 5: Fun With Diffusion</h1>
			<div style="text-align: center;">Name: Alexander Waldman </div>
			<div style="text-align: center;"><a href = "../">‚Üê Back to Homepage</a></div>

      <h1>Part A: The Power of Diffusion Models!</h1>

      <h2>Part 0: Play with the Model using Your Own Text Prompts!</h2>

      <p>To begin this project, I wanted to see what kind of images the DeepFloyd IF diffusion model was capable of producing. Using the prompt embeddings provided in the starter code, I generated images with the embeddings that correspond to "an oil painting of people around a campfire", "an oil painting of an old man", and "a photo of a hipster barista". While they aren't the highest quality generated images I've ever seen, I'd say the resulting images fit the prompts very well! I also messed around with a few different values for the number of inference steps in each stage of the model. The first 3 images were generated with 20 inference steps in each stage. For the next 3 images, I re-ran the model with the prompt "an oil painting of people around a campfire" using 40, 80, and 160 inference steps respectively. It seems that taking more inference steps corresponds to more fine-grained details in the output image. I should note that all images for this section and following sections were created using a <b>seed value = 123</b></p>
      <div class="row">
				<div class="column2" style="text-align: center;">
					<img src="./images/part0_1.png" alt="" style="width:100%">
					<figcaption>an oil painting of people around a campfire</figcaption>
				</div>
				<div class="column2" style="text-align: center;">
					<img src="./images/part0_2.png" alt="" style="width:100%">
					<figcaption>an oil painting of an old man</figcaption>
				</div>
				<div class="column2" style="text-align: center;">
					<img src="./images/part0_3.png" alt="" style="width:100%">
					<figcaption>a photo of a hipster barista</figcaption>
				</div>
			</div>
			<div class="row">
				<div class="column2" style="text-align: center;">
					<img src="./images/part0_4.png" alt="" style="width:100%">
					<figcaption>num_inference_steps = 40</figcaption>
				</div>
				<div class="column2" style="text-align: center;">
					<img src="./images/part0_5.png" alt="" style="width:100%">
					<figcaption>num_inference_steps = 80</figcaption>
				</div>
				<div class="column2" style="text-align: center;">
					<img src="./images/part0_6.png" alt="" style="width:100%">
					<figcaption>num_inference_steps = 160</figcaption>
				</div>
			</div>

      <h2>Part 1: Sampling Loops</h2>

      <h2>1.1 Implementing the Forward Process</h2>
	  <p>As mentioned in the spec, the forward process is a crucial part of the diffusion process where we iteratively add noise to a clean image. From there, the model can try to predict a denoised version of the image step-by-step until it arrives at a newly generated clean picture. The forward process can be represented as $$ x_t = \sqrt{\bar\alpha_t} x_0 + \sqrt{1 - \bar\alpha_t} \epsilon \quad \text{where}~ \epsilon \sim N(0, 1)
		$$ where x<sub>0</sub> is our original image, x<sub>t</sub> is the noisy image, \( \bar\alpha_t \) is the noise coefficient corresponding to noise level <code>t</code>, and &epsilon is the noise itself. In order to create an image at a specified noise level <code>t</code>, I implemented <code>forward()</code>. The key lines of my implementation are the following:</p>
<pre>
  <code>
    eps = torch.randn_like(im)
    im_noisy = (torch.sqrt(alphas_cumprod[t]) * im) + (torch.sqrt(1 - alphas_cumprod[t]) * eps)
  </code>
</pre>
	<p>With <code>forward()</code> working, I could produce images of the Campanile at noise level [250, 500, 750].</p>
      <div class="row">
				<div class="column3" style="text-align: center;">
					<img src="./images/part1_1_1.png" alt="" style="width:256px">
					<figcaption>Original</figcaption>
				</div>
				<div class="column3" style="text-align: center;">
					<img src="./images/part1_1_2.png" alt="" style="width:256px">
					<figcaption>Noisy Campanile at t=250</figcaption>
				</div>
				<div class="column3" style="text-align: center;">
					<img src="./images/part1_1_3.png" alt="" style="width:256px">
					<figcaption>Noisy Campanile at t=500</figcaption>
				</div>
        		<div class="column3" style="text-align: center;">
					<img src="./images/part1_1_4.png" alt="" style="width:256px">
					<figcaption>Noisy Campanile at t=750</figcaption>
				</div>
		</div>
      
      <h2>1.2 Classical Denoising</h2>
      <p>A common strategy for denoising images is to apply Gaussian blur in order to smooth out pixel variations. Unfortunately, simply blurring a noisy image is not enough to recover a clean image from a noisy one. Looking at the images below, you'll see that blurring the noisy images from section 1.1 (kernel size = 5) does not acheive good results.</p>

      <div class="row">
				<div class="column2" style="text-align: center;">
					<img src="./images/part1_1_2.png" alt="" style="width:256px">
					<figcaption>Noisy Campanile at t=250</figcaption>
				</div>
				<div class="column2" style="text-align: center;">
					<img src="./images/part1_1_3.png" alt="" style="width:256px">
					<figcaption>Noisy Campanile at t=500</figcaption>
				</div>
				<div class="column2" style="text-align: center;">
					<img src="./images/part1_1_4.png" alt="" style="width:256px">
					<figcaption>Noisy Campanile at t=750</figcaption>
				</div>
			</div>
      <div class="row">
				<div class="column2" style="text-align: center;">
					<img src="./images/part1_2_1.png" alt="" style="width:256px">
					<figcaption>Gaussian Blur Denoising at t=250</figcaption>
				</div>
				<div class="column2" style="text-align: center;">
					<img src="./images/part1_2_2.png" alt="" style="width:256px">
					<figcaption>Gaussian Blur Denoising at t=500</figcaption>
				</div>
				<div class="column2" style="text-align: center;">
					<img src="./images/part1_2_3.png" alt="" style="width:256px">
					<figcaption>Gaussian Blur Denoising at t=750</figcaption>
				</div>
			</div>
      <h2>1.3 One-Step Denoising</h2>
	  <p>Moving on from classical denoising, we can try to use a pretrained diffusion model to recover clean images from noisy ones. We are using a UNet that has been trained to recover the Gaussian noise from a noisy image. It was also trained with text conditioning, meaning we can provide a prompt embedding to inform the model what kind of images we want it to generate. For this section, I used the prompt embedding corresponding to "a high quality photo". After estimating the amount of noise in an image, we can plug it into the equation from section 1.1 as &epsilon; and solve for x<sub>0</sub>, treating the noisy image as x<sub>t</sub>. This gives us an estimate of what the original image looks like. Below, I've visualzied the original image, the Campanile at each noise level, and the estimate of the orginal image at each noise level. As we introduce more noise to the image, the result of one-step denoising gets further and further from the original.</p>
      <div class ="row">
				<div class="column4" style="text-align: center;">
					<img src="./images/part1_1_1.png" alt="" style="width:256px">
					<figcaption>Original</figcaption>
				</div>        
      </div>
      <div class="row">
				<div class="column2" style="text-align: center;">
					<img src="./images/part1_1_2.png" alt="" style="width:256px">
					<figcaption>Noisy Campanile at t=250</figcaption>
				</div>
				<div class="column2" style="text-align: center;">
					<img src="./images/part1_1_3.png" alt="" style="width:256px">
					<figcaption>Noisy Campanile at t=500</figcaption>
				</div>
				<div class="column2" style="text-align: center;">
					<img src="./images/part1_1_4.png" alt="" style="width:256px">
					<figcaption>Noisy Campanile at t=750</figcaption>
				</div>
			</div>
            <div class="row">
				<div class="column2" style="text-align: center;">
					<img src="./images/part1_3_1.png" alt="" style="width:256px">
					<figcaption>One-Step Denoised Campanile at t=250</figcaption>
				</div>
				<div class="column2" style="text-align: center;">
					<img src="./images/part1_3_2.png" alt="" style="width:256px">
					<figcaption>One-Step Denoised Campanile at t=500</figcaption>
				</div>
				<div class="column2" style="text-align: center;">
					<img src="./images/part1_3_3.png" alt="" style="width:256px">
					<figcaption>One-Step Denoised Campanile at t=750</figcaption>
				</div>
			</div>

      <h2>1.4 Iterative Denoising</h2>
	  <p>Instead of trying to denoise an image in a single step, it makes more sense to denoise an image iteratively, as that's what diffusion models are trained to do. We can do this by iterating over a list of monotonically decreasing noise levels, denoising between each level until we reach the final, clean image. I created a list of strided noise levels <code>strided_timestamps</code> that begins at 990 and takes steps of 30 until we reach 0. As described in the spec:</p>
	  <p>On the <code>i</code>th denoising step we are at <code>t = strided_timesteps[i]</code>, and want to get to <code>t = strided_timesteps[i+1]</code> (from more noisy to less noisy). To actually do this, we have the following formula:

$$ x_{t'} = \frac{\sqrt{\bar\alpha_{t'}}\beta_t}{1 - \bar\alpha_t} x_0 + \frac{\sqrt{\alpha_t}(1 - \bar\alpha_{t'})}{1 - \bar\alpha_t} x_t + v_\sigma $$

where x<sub>t</sub> is your image at timestep <code>t</code>, x<sub>t'</sub> is your noisy image at timestep <code>t'</code>, \( \bar\alpha_t \) is the noise coefficient, \( \alpha_t = \bar\alpha_t / \bar\alpha_{t'} \), \( \beta_t = 1 - \alpha_t \), \( x_0 \) is our current estimate of the clean image, and \( v_\sigma \) is the predicted noise provided by <code>add_variance()</code> </p>
		<p> I implemented this functionality by filling in the <code>iterative_denoise</code> function, which loops over each timestep/noise level in <code>strided_timestamps</code>, gradually denoising the image until it reaches level 0. The key piece of code from this function is my implementation of the equation above, which looks like:</p>
<pre>
  <code>
x_0 = (image - (noise_est * torch.sqrt(1 - alpha_cumprod))) / torch.sqrt(alpha_cumprod)
t1 = ((torch.sqrt(alpha_cumprod_prev) * beta) / (1 - alpha_cumprod)) * x_0
t2 = ((torch.sqrt(alpha) * (1 - alpha_cumprod_prev)) / (1 - alpha_cumprod)) * image
v_sigma = add_variance(predicted_variance, prev_t, image) - image
pred_prev_image = t1 + t2 + v_sigma
image = pred_prev_image
  </code>
</pre>

<p>With <code>iterative_denoise</code> implemented, I could gradually denoise a very noisy image of the Campanile and compare it to the output of our previous denoising methods. As you can see, iterative denoising resulted in a sharper, more detailed output than the other methods.</p>
      <div class="row">
			<div class="column5" style="text-align: center;">
				<img src="./images/part1_4_5.png" alt="" style="width:256px">
				<figcaption>Noisy Campanile at t=90</figcaption>
			</div>
			<div class="column5" style="text-align: center;">
				<img src="./images/part1_4_4.png" alt="" style="width:256px">
				<figcaption>Noisy Campanile at t=240</figcaption>
			</div>
			<div class="column5" style="text-align: center;">
				<img src="./images/part1_4_3.png" alt="" style="width:256px">
				<figcaption>Noisy Campanile at t=390</figcaption>
			</div>
			<div class="column5" style="text-align: center;">
				<img src="./images/part1_4_2.png" alt="" style="width:256px">
				<figcaption>Noisy Campanile at t=540</figcaption>
			</div>
							<div class="column5" style="text-align: center;">
				<img src="./images/part1_4_1.png" alt="" style="width:256px">
				<figcaption>Noisy Campanile at t=690</figcaption>
			</div>
		</div>
		      <div class="row">
				<div class="column3" style="text-align: center;">
					<img src="./images/part1_1_1.png" alt="" style="width:256px">
					<figcaption>Original</figcaption>
				</div>
				<div class="column3" style="text-align: center;">
					<img src="./images/part1_4_6.png" alt="" style="width:256px">
					<figcaption>Iteratively Denoised Campanile</figcaption>
				</div>
				<div class="column3" style="text-align: center;">
					<img src="./images/part1_4_7.png" alt="" style="width:256px">
					<figcaption>One-Step Denoised Campanile</figcaption>
				</div>
        		<div class="column3" style="text-align: center;">
					<img src="./images/part1_4_8.png" alt="" style="width:256px">
					<figcaption>Gaussian Blurred Campanile</figcaption>
				</div>
		</div>

      <h2>1.5 Diffusion Model Sampling</h2>
	  <p>Now that iterative denoising works, we can try to generate images from scratch by feeding the model pure noise instead of a noisy version of the Campanile. I'm also still using the embedding for "a high quality photo" as a prompt for the model. Shown below are a few samples of the model's output under these conditions.</p>
      <div class="row">
			<div class="column5" style="text-align: center;">
				<img src="./images/part1_5_1.png" alt="" style="width:256px">
				<figcaption>Sample 1</figcaption>
			</div>
			<div class="column5" style="text-align: center;">
				<img src="./images/part1_5_2.png" alt="" style="width:256px">
				<figcaption>Sample 2</figcaption>
			</div>
			<div class="column5" style="text-align: center;">
				<img src="./images/part1_5_3.png" alt="" style="width:256px">
				<figcaption>Sample 3</figcaption>
			</div>
			<div class="column5" style="text-align: center;">
				<img src="./images/part1_5_4.png" alt="" style="width:256px">
				<figcaption>Sample 4</figcaption>
			</div>
							<div class="column5" style="text-align: center;">
				<img src="./images/part1_5_5.png" alt="" style="width:256px">
				<figcaption>Sample 5</figcaption>
			</div>
		</div>
      <h2>1.6 Classifier-Free Guidance (CFG)</h2>
	  <p>While the generated images from section 1.5 are pretty cool, they're not exceptionally high quality. The samples from above are some of the <b>best</b> examples of the model generating images from scratch. After running the model a few times, it was not uncommon for it to generate images with completely indistinguishable features. One way to improve the model's performance is to integrate classifier free guidance (CFG). In CFG, the noise estimate we use to denoise is conditioned on a text prompt as well as an unconditional noise estimate. Then, the overall noise estimate used to denoise the current best estimate of the clean image is</p>
	  $$\epsilon = \epsilon_u + \gamma (\epsilon_c - \epsilon_u)$$
	  <p>When &gamma; is greater than 1, we get <b>much</b> better results from our model. Implementing CFG did not require too many changes from the code I wrote for section 1.4. The main difference is that I had to run the model twice to get the conditional noise estimate corresponding to "a high quality photo" and the unconditional noise estimate corresponding to the null prompt "". From there, I could calculate the CFG noise using the equation above and use it to denoise the image. </p>
<pre>
  <code>
cfg_noise = uncond_noise_est + scale * (noise_est - uncond_noise_est)
x_0 = (image - (cfg_noise * torch.sqrt(1 - alpha_cumprod))) / torch.sqrt(alpha_cumprod)
...
  </code>
</pre>	
<p>As you can see, the model is now able to produce much more impressive images!</p>
	  <div class="row">
			<div class="column5" style="text-align: center;">
				<img src="./images/part1_6_1.png" alt="" style="width:256px">
				<figcaption>Sample 1 with CFG</figcaption>
			</div>
			<div class="column5" style="text-align: center;">
				<img src="./images/part1_6_2.png" alt="" style="width:256px">
				<figcaption>Sample 2 with CFG</figcaption>
			</div>
			<div class="column5" style="text-align: center;">
				<img src="./images/part1_6_3.png" alt="" style="width:256px">
				<figcaption>Sample 3 with CFG</figcaption>
			</div>
			<div class="column5" style="text-align: center;">
				<img src="./images/part1_6_4.png" alt="" style="width:256px">
				<figcaption>Sample 4 with CFG</figcaption>
			</div>
			<div class="column5" style="text-align: center;">
				<img src="./images/part1_6_5.png" alt="" style="width:256px">
				<figcaption>Sample 5 with CFG</figcaption>
			</div>
		</div>
      <h2>1.7 Image-to-image Translation</h2>
	  <p>We can now examine how different our resulting images our depending on the amount of noise we add to the original image. In theory, when we add more noise to the original, the generated image should be further from the ground truth and vice versa. To do this, we can add noise to our input images and then run <code>iterative_denoise_cfg</code> starting at different timestamps. When <code>i_start</code> is larger, we are starting at a lower <code>t</code> value (less noise), meaning the result should be a bit closer to the original. To test this, I ran <code>iterative_denoise_cfg</code> at noise levels [1, 3, 5, 7, 10, 20] with the conditional text prompt "a high quality photo" on the Campanile, the apple from project 2, and the emir from project 1. We can see that the images gradually look more like the original, especially starting around level 7 or 10.</p>
		<div class="row">
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_1.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=1</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_2.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=3</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_3.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=5</figcaption>
				</div>
        		<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_4.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=7</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_5.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=10</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_6.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=20</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_7.png" alt="" style="width:256px">
					<figcaption>Apple</figcaption>
				</div>
			</div>
				<div class="row">
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_8.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=1</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_9.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=3</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_10.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=5</figcaption>
				</div>
        		<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_11.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=7</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_12.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=10</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_13.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=20</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_14.png" alt="" style="width:256px">
					<figcaption>Apple</figcaption>
				</div>
			</div>
				<div class="row">
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_15.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=1</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_16.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=3</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_17.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=5</figcaption>
				</div>
        		<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_18.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=7</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_19.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=10</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_20.png" alt="" style="width:256px">
					<figcaption>SDEdit with i_start=20</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_7_21.png" alt="" style="width:256px">
					<figcaption>Emir of Bukhara</figcaption>
				</div>
			</div>
      <h3>1.7.1 Editing Hand-Drawn and Web Images</h3>
	  <p>Continuing our exploration of image-to-image translation, we can try to use hand-drawn inputs to see how they're projected onto the natural image manifold. For this section, I made 2 sketches and found an image from the web. As you can see, the model seems to do a bit better at predicting my sketches than the pizza when <code>i_start</code> is smaller (probably due to the simplicity of the sketches). </p>
		<div class="row">
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_1.png" alt="" style="width:256px">
					<figcaption>Pizza at i_start=1</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_2.png" alt="" style="width:256px">
					<figcaption>Pizza at i_start=3</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_3.png" alt="" style="width:256px">
					<figcaption>Pizza at i_start=5</figcaption>
				</div>
        		<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_4.png" alt="" style="width:256px">
					<figcaption>Pizza at i_start=7</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_5.png" alt="" style="width:256px">
					<figcaption>Pizza at i_start=10</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_6.png" alt="" style="width:256px">
					<figcaption>Pizza at i_start=20</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_7.png" alt="" style="width:256px">
					<figcaption>Pizza</figcaption>
				</div>
		</div>
		<div class="row">
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_8.png" alt="" style="width:256px">
					<figcaption>Sailboat at i_start=1</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_9.png" alt="" style="width:256px">
					<figcaption>Sailboat at i_start=3</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_10.png" alt="" style="width:256px">
					<figcaption>Sailboat at i_start=5</figcaption>
				</div>
        		<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_11.png" alt="" style="width:256px">
					<figcaption>Sailboat at i_start=7</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_12.png" alt="" style="width:256px">
					<figcaption>Sailboat at i_start=10</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_13.png" alt="" style="width:256px">
					<figcaption>Sailboat at i_start=20</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_14.png" alt="" style="width:256px">
					<figcaption>Original Sailboat Sketch</figcaption>
				</div>
		</div>
		<div class="row">
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_15.png" alt="" style="width:256px">
					<figcaption>Apple at i_start=1</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_16.png" alt="" style="width:256px">
					<figcaption>Apple at i_start=3</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_17.png" alt="" style="width:256px">
					<figcaption>Apple at i_start=5</figcaption>
				</div>
        		<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_18.png" alt="" style="width:256px">
					<figcaption>Apple at i_start=7</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_19.png" alt="" style="width:256px">
					<figcaption>Apple at i_start=10</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_20.png" alt="" style="width:256px">
					<figcaption>Apple at i_start=20</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_71_21.png" alt="" style="width:256px">
					<figcaption>Original Apple Sketch</figcaption>
				</div>
		</div>
      <h3>1.7.2 Inpainting</h3>
	  <p>Next, we can try to have the model do some inpainting! To do this, we can create a mask that isolates the area of the image we want to change. From there, after every diffusion step, we set all of the pixels where the mask is 0 equal to pixels from the original photo. For this task, I essentially copied and pasted my implementation for <code>iterative_denoise_cfg</code> being this line of code</p>
<pre>
  <code>
    image = (image * mask) + ((1-mask) * forward(original_image, prev_t).half().to(device))
  </code>
</pre>		  
<p>which sets masked out pixels equal to corresponding pixels from the original image at the end of each diffusion step. Below, I have some examples of inpainting with pictures of the Campanile, Steph Curry, and Klay Thompson.</p>
		<div class="row">
			<div class="column3" style="text-align: center;">
				<img src="./images/part1_72_1.png" alt="" style="width:256px">
				<figcaption>Campanile</figcaption>
			</div>
			<div class="column3" style="text-align: center;">
				<img src="./images/part1_72_2.png" alt="" style="width:256px">
				<figcaption>Mask</figcaption>
			</div>
			<div class="column3" style="text-align: center;">
				<img src="./images/part1_72_3.png" alt="" style="width:256px">
				<figcaption>Hole to Fill</figcaption>
			</div>
			<div class="column3" style="text-align: center;">
				<img src="./images/part1_72_4.png" alt="" style="width:256px">
				<figcaption>Campanile Inpainted</figcaption>
			</div>
		</div>
				<div class="row">
			<div class="column3" style="text-align: center;">
				<img src="./images/part1_72_5.png" alt="" style="width:256px">
				<figcaption>Campanile</figcaption>
			</div>
			<div class="column3" style="text-align: center;">
				<img src="./images/part1_72_6.png" alt="" style="width:256px">
				<figcaption>Mask</figcaption>
			</div>
			<div class="column3" style="text-align: center;">
				<img src="./images/part1_72_7.png" alt="" style="width:256px">
				<figcaption>Hole to Fill</figcaption>
			</div>
			<div class="column3" style="text-align: center;">
				<img src="./images/part1_72_8.png" alt="" style="width:256px">
				<figcaption>Steph Curry Inpainted</figcaption>
			</div>
		</div>
				<div class="row">
			<div class="column3" style="text-align: center;">
				<img src="./images/part1_72_9.png" alt="" style="width:256px">
				<figcaption>Campanile</figcaption>
			</div>
			<div class="column3" style="text-align: center;">
				<img src="./images/part1_72_10.png" alt="" style="width:256px">
				<figcaption>Mask</figcaption>
			</div>
			<div class="column3" style="text-align: center;">
				<img src="./images/part1_72_11.png" alt="" style="width:256px">
				<figcaption>Hole to Fill</figcaption>
			</div>
			<div class="column3" style="text-align: center;">
				<img src="./images/part1_72_12.png" alt="" style="width:256px">
				<figcaption>Klay Thompson Inpainted</figcaption>
			</div>
		</div>
      <h3>1.7.3 Text-Conditional Image-to-image Translation</h3>
	  <p>In this task, I wanted to see how the model would react if I changed the text prompt from "a high quality photo" to something that gives me a but more control over the output. For the following series of images, I used the prompt embedding corresponding to "a pencil" to guide the diffusion process. As you can see, the output images gradually look more like the original but still retain characteristics from the text prompt. </p>
		<div class="row">
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_1.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 1</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_2.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 3</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_3.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 5</figcaption>
				</div>
        		<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_4.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 7</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_5.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 10</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_6.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 20</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_7.png" alt="" style="width:256px">
					<figcaption>Campanile</figcaption>
				</div>
		</div>
		<div class="row">
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_8.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 1</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_9.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 3</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_10.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 5</figcaption>
				</div>
        		<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_11.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 7</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_12.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 10</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_13.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 20</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_14.png" alt="" style="width:256px">
					<figcaption>Sword</figcaption>
				</div>
		</div>
		<div class="row">
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_15.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 1</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_16.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 3</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_17.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 5</figcaption>
				</div>
        		<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_18.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 7</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_19.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 10</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_20.png" alt="" style="width:256px">
					<figcaption>Pencil at noise level 20</figcaption>
				</div>
				<div class="column6" style="text-align: center;">
					<img src="./images/part1_73_21.png" alt="" style="width:256px">
					<figcaption>Banana</figcaption>
				</div>
		</div>
      <h2>1.8 Visual Anagrams</h2>
	  <p>Now, I wanted to try and create visual anagrams. The key behind doing this is to first pass 2 different prompts to the model. Each diffusion step, find the conditional and unconditional noise estimates for the first prompt. Then, flip the image and find the noise estimates using the other prompt. From there, you can apply an average noise estimate to x<sub>t</sub>, allowing both prompts to show up in the final image. The formula for finding the average noise estimate is as follows:</p>
	  $$ \epsilon_1 = \text{CFG of UNet}(x_t, t, p_1) $$
$$ \epsilon_2 = \text{flip}(\text{CFG of UNet}(\text{flip}(x_t), t, p_2))$$
$$ \epsilon = (\epsilon_1 + \epsilon_2) / 2 $$
<p>In code, I implemented the formula like this. I'll note that I also decided to used the average of the predicted variance for the normal and flipped version since that seemed like a reasonable approach.</p>
<pre>
  <code>
eps_1 = uncond_noise_est + scale * (noise_est - uncond_noise_est)
eps_2 = torch.flip(uncond_noise_est_flipped + scale * (noise_est_flipped - uncond_noise_est_flipped), dims=[2])
eps = (eps_1 + eps_2) / 2
predicted_variance_avg = (predicted_variance + torch.flip(predicted_variance_flipped, dims=[2])) / 2
x_0 = (image - (eps * torch.sqrt(1 - alpha_cumprod))) / torch.sqrt(alpha_cumprod)
t1 = ((torch.sqrt(alpha_cumprod_prev) * beta) / (1 - alpha_cumprod)) * x_0
t2 = ((torch.sqrt(alpha) * (1 - alpha_cumprod_prev)) / (1 - alpha_cumprod)) * image
v_sigma = add_variance(predicted_variance_avg, prev_t, image) - image
pred_prev_image = t1 + t2 + v_sigma
image = pred_prev_image
</code>
</pre>	
<p>With <code>make_flip_illusion</code> working, I was able to produce these visual illusions!</p>
<div class="row">
				<div class="column
				" style="text-align: center;">
					<img src="./images/part1_8_1.png" alt="" style="width:256px">
					<figcaption>An Oil Painting of an Old Man</figcaption>
				</div>
				<div class="column" style="text-align: center;">
					<img src="./images/part1_8_2.png" alt="" style="width:256px">
					<figcaption>An Oil Painting of People around a Campfire</figcaption>
				</div>
		</div>
		<div class="row">
				<div class="column" style="text-align: center;">
					<img src="./images/part1_8_3.png" alt="" style="width:256px">
					<figcaption>An Oil Painting of a Snowy Mountain Village</figcaption>
				</div>
				<div class="column" style="text-align: center;">
					<img src="./images/part1_8_4.png" alt="" style="width:256px">
					<figcaption>A Man Wearing a Hat</figcaption>
				</div>
		</div>
      <h2>1.9 Hybrid Images</h2>
	  <p>The last task was to create hybrid images that look like one thing from close up but another from far away. The process for doing this is not that different than the process for making visual anagrams. The algorithm for finding the combined noise estimate of two prompts in a hybrid image is</p>
$$ \epsilon_1 = \text{CFG of UNet}(x_t, t, p_1) $$

$$ \epsilon_2 = \text{CFG of UNet}(x_t, t, p_2) $$

$$ \epsilon = f_\text{lowpass}(\epsilon_1) + f_\text{highpass}(\epsilon_2)$$	
<p>In my implementation of <code>make_hybrid</code>, I found the CFG noise for the image for both prompts. I applied a low pass filter to the first noise estimate by blurring it. Then, I applied a high pass filter to the second noise estimate by subtracting a blurred version of the noise from itself. Lastly, I added them together to get the combined noise. I did the same to combine the predicted variance as well. In code, it looks like this:</p>
<pre>
  <code>
eps_1 = uncond_noise_est + scale * (noise_est_low - uncond_noise_est)
eps_2 = uncond_noise_est + scale * (noise_est_high - uncond_noise_est)
eps = TF.gaussian_blur(eps_1, kernel_size=33, sigma=2) + (eps_2 - TF.gaussian_blur(eps_2, kernel_size=33, sigma=2))
predicted_variance = TF.gaussian_blur(predicted_variance_low, kernel_size=33, sigma=2) 
+ (predicted_variance_high - TF.gaussian_blur(predicted_variance_high, kernel_size=33, sigma=2))
...
</code>
</pre>
<p>Now we have hybrid images! The first image appears to depict a waterfall, but if you move far away, you'll see a skull. Similarly, the second image looks like a double-sided pencil, but if you move away, you'll see a rocket blasting off!  </p>	
	  <div class="row">
				<div class="column" style="text-align: center;">
					<img src="./images/part1_9_1.png" alt="" style="width:256px">
					<figcaption>Hybrid image of a skull and a waterfall</figcaption>
				</div>
				<div class="column" style="text-align: center;">
					<img src="./images/part1_9_2.png" alt="" style="width:256px">
					<figcaption>Hybrid image of a rocket ship and a pencil</figcaption>
				</div>
		</div>
	<h1>Part B: Flow Matching From Scratch!</h1>

	<h2>Part 1: Training a Single-Step Denoising UNet</h2>

	<h2>1.1 Implementing the UNet</h2>
	<p>To begin part 1, I needed to construct a UNet, a one-step denoiser that can map a noisy image to a clean one. My implementation of the UNet follows the diagram below:</p>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_11_1.png" alt="" style="width:80%">
				<figcaption>UNet Architecture</figcaption>
			</div>
		</div>
	<h2>1.2 Using the UNet to Train a Denoiser</h2>
	<p>Before I could begin trainig the UNet to denoise images, I needed to be able to create noisy images from clean ones. I did this using the following equation
			$$z = x + \sigma \epsilon,\quad \text{where }\epsilon \sim N(0, I). \tag{B.2}$$ where x is the clean image, z is the noisy image, and &sigma; is the noise level. Pictured below are some examples of an image from the MNIST with different levels of noise applied.</p>
		<div class="row">
			<div class="column6" style="text-align: center;">
				<img src="./images/part2_12_1.png" alt="" style="width:256px">
				<figcaption>&sigma; = 0.0</figcaption>
			</div>
			<div class="column6" style="text-align: center;">
				<img src="./images/part2_12_2.png" alt="" style="width:256px">
				<figcaption>&sigma; = 0.2</figcaption>
			</div>
			<div class="column6" style="text-align: center;">
				<img src="./images/part2_12_3.png" alt="" style="width:256px">
				<figcaption>&sigma; = 0.4</figcaption>
			</div>
			<div class="column6" style="text-align: center;">
				<img src="./images/part2_12_4.png" alt="" style="width:256px">
				<figcaption>&sigma; = 0.5</figcaption>
			</div>
			<div class="column6" style="text-align: center;">
				<img src="./images/part2_12_5.png" alt="" style="width:256px">
				<figcaption>&sigma; = 0.6</figcaption>
			</div>
			<div class="column6" style="text-align: center;">
				<img src="./images/part2_12_6.png" alt="" style="width:256px">
				<figcaption>&sigma; = 0.8</figcaption>
			</div>
			<div class="column6" style="text-align: center;">
				<img src="./images/part2_12_7.png" alt="" style="width:256px">
				<figcaption>&sigma; = 1.0</figcaption>
			</div>
		</div>	
	<h3>1.2.1 Training</h3>
	<p>The next stage involved training my UNet architecture to denoise images with a noise level of 0.5 applied to them. Using the skeleton code provided in the section of the notebook, I wrote a training loop for my UNet that ran for 5 epochs. I also used the same hyperparameters mentioned in the spec (batch size of 256, 128 hidden dimensions, Adam optimizer with learning rate of 1e-4). Pictured below are examples of clean images from the test set, noisy inputs to the model, and the outputs of my model after training for 1 and 5 epochs. </p>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_121_1.png" alt="" style="width:80%">
				<figcaption>Clean Images</figcaption>
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_121_2.png" alt="" style="width:80%">
				<figcaption>Noisy Images (&sigma; = 0.5)</figcaption>
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_121_3.png" alt="" style="width:80%">
				<figcaption>Denoised Images After 1 Epoch</figcaption>
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_121_4.png" alt="" style="width:80%">
				<figcaption>Denoised Images After 5 Epochs</figcaption>
			</div>
		</div>
		<p>As you can see, The model is able to recover clean images from noisy ones very effectively after just 5 epochs! I've also included a plot of my model's training loss, and you'll notice it does a vast majority of the learning within the first couple hundred iterations (epoch 1) before slowly refining itself over the course of the rest of its training.</p>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_121_5.png" alt="" style="width:80%">
			</div>
		</div>
	<h3>1.2.2 Out-of-Distribution Testing</h3>
	<p>Next, we can test how our model performs when it's ask to denoise images with noise levels it was not trained on. Below, I've visualzied the output of my model on digits with noise levels 0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0. Since the model was trained to denoise images with noise level 0.5, its not surprising that it's able to denoise images at lower noise levels as well. I am pretty impressed with how it performed at noise level 0.6 since it was not trained to handle images that noisy. Once the noise level hits 0.8 and 1.0, it can be very difficult for even a human to distinguish the digit being displayed, so I'm not shocked that the output of the model gets very messy at those levels.
	</p>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_122_1.png" alt="" style="width:80%">
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_122_2.png" alt="" style="width:80%">
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_122_3.png" alt="" style="width:80%">
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_122_4.png" alt="" style="width:80%">
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_122_5.png" alt="" style="width:80%">
			</div>
		</div>
	<h3>1.2.3 Denoising Pure Noise</h3>
	<p>In order to have out model be generative, we can have it try to denoise pure noise. Every epoch, we can have the model try to denoise pure Gaussian noise but still calculate loss against clean images of the digits. Pictured below are some sample results on pure noise after the first and the 5-th epoch.</p>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_123_1.png" alt="" style="width:80%">
				<figcaption>Pure Noise</figcaption>
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_123_2.png" alt="" style="width:80%">
				<figcaption>Denoised Images After 1 Epoch</figcaption>
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_123_3.png" alt="" style="width:80%">
				<figcaption>Denoised Images After 5 Epochs</figcaption>
			</div>
		</div>
		<p>One thing that immediately sticks out to me is the blob-like pattern in the middle of the generated outputs. You'll also notice that the images don't get much cleaner from the first to the fifth epoch. Since we are trying to denoise pure noise, there is no underlying digit for the model to uncover. As a result, the model learns to predict where digits are located within the image instead of specific digits, which is why the blobs look a bit like the numbers 0-9 stacked on top of each other. Since pure noise has no inherent pattern, the best the model can do when denoising is to guess where a digit is most likely to be. This is also the reason that the training loss doesn't continue to shrink after initially decreasing.   </p>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_123_4.png" alt="" style="width:80%">
			</div>
		</div>

	<h2>Part 2: Training a Flow Matching Model</h2>
	
	<h2>2.1 Adding Time Conditioning to UNet</h2>
	<p>As shown in part 1, a one-step denoising architecture does not work well for generative tasks. One change we can make is to use flow matching, where the model denoises an image step-by-step by trying to predictthe `flow' from noisy data to clean data. In order to train our model to denoise step-by-step, it needs to know what the images look like in between noisy and clean. We will do this by simply linearly interpolating between noisy and clean images, using a scalar t to determine how exactly they are combined. To integrate this feature into my model, I slightly augmented the original UNet archtecture by adding to FCBlocks (fully-connected blocks) that take t as an input. From there I made slight changes to the initialization and forward fucntion of the original UNet to accomodate the new FCBlocks. Additionally, I followed Algorithm 1 and 2 from the spec to implement a training and sampling function for my new time-conditioned UNet</p>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_21_1.png" alt="" style="width:80%">
				<figcaption>Time-Conditioned UNet Architecture</figcaption>
			</div>
		</div>
	<h2>2.2 Training the UNet</h2>
	<p>Next, I trained my time-conditioned UNet to produce clean digits from pure noise. The code for this section was very similar to the training loop I implemented for the UNet, with a few slight changes in the hyperparameters (batch size of 64, 64 hidden dimensions, Adam optimizer with learning rate of 1e-2) and the addition of an exponential learning rate scheduler. Shown below is the loss curve that resulted from training my model for 10 epochs.</p>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_22_1.png" alt="" style="width:80%">
			</div>
		</div>
	<h2>2.3 Sampling from the UNet</h2>
	<p>Using the sampling function I implemented earlier, we can see how well my model manages to generate digits from pure noise. Below, I have samples of the output from my time-conditioned UNet after training for 1, 5, and 10 epochs. Although the outputs are pretty messy, the results do improve over time. Over time, my model learned to produce a good amount discernable (albeit sloppy) digits from pure noise! </p>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_23_1.png" alt="" style="width:80%">
				<figcaption>1 Epoch</figcaption>
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_23_2.png" alt="" style="width:80%">
				<figcaption>5 Epochs</figcaption>
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_23_3.png" alt="" style="width:80%">
				<figcaption>10 Epochs</figcaption>
			</div>
		</div>
	<h2>2.4 Adding Class-Conditioning to UNet</h2>
<p>The next improvement we can add onto out model is to condition our outputs on the class of the digit 0-9, which involved adding two more FCBlocks to the model that take in a one-hot-encoded vector representing the class. Following the example in the spec, I altered my model to be conditioned on both time t and class c.</p>
	<h2>2.5 Training the UNet</h2>
	<p>With class-conditioning added to my UNet, I implemented Algorithm 5 and 6 from the spec for training and sampling. Then, I retrained the model using the a very similar training loop as before but with class labels passed in as an additional argument. Below is the loss curve that resulted from training my class-conditioned UNet for 10 epochs.</p>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_25_1.png" alt="" style="width:80%">
			</div>
		</div>
	<h2>2.6 Sampling from the UNet</h2>
	<p>Finally, we can see how well our class-conditioned UNet is able to generate digits from pure noise when given a specific class label. Below, I have samples of the output from my class-conditioned UNet after training for 1, 5, and 10 epochs. The results are a significant improvement over the time-conditioned UNet! After just 5 epochs, the model is able to produce fairly clear digits that correspond to the given class label. By epoch 10, the outputs are very clean and easily recognizable as digits 0-9. Overall, adding class-conditioning greatly improved the generative capabilities of my model. </p>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_26_1.png" alt="" style="width:80%">
				<figcaption>1 Epoch</figcaption>
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_26_2.png" alt="" style="width:80%">
				<figcaption>5 Epochs</figcaption>
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_26_3.png" alt="" style="width:80%">
				<figcaption>10 Epochs</figcaption>
			</div>
		</div>
	<p>Another factor to consider when making these models is the simplicity of our code. To avoid having to deal with a rate scheduler, I tried to reproduce the results from above by simply reducing the learning rate of my Adam optimizer from 0.01 to 0.001. Since the rate scheduler lowers the learning rate over time anyways, I figured I could compensate for the loss of the scheduler by starting at a low learning rate. Below I have samples of the output from my class-conditioned UNet after training for 1, 5, and 10 epochs without a learning rate scheduler. I think the results look pretty good! By the 10th epoch, the quality of the outputs is essentially the same. Unfortunately, the training process took much longer for this version of the model since the learning rate made gradient updates much smaller.</p>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_26_4.png" alt="" style="width:80%">
				<figcaption>1 Epoch (no rate scheduler)</figcaption>
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_26_5.png" alt="" style="width:80%">
				<figcaption>5 Epochs (no rate scheduler)</figcaption>
			</div>
		</div>
	  	<div class="row">
			<div class="column7" style="text-align: center;">
				<img src="./images/part2_26_6.png" alt="" style="width:80%">
				<figcaption>10 Epochs (no rate scheduler)</figcaption>
			</div>
		</div>	
</body>
</html>

<div class="lightbox">
    <img src="" alt="Expanded view">  </div>
<script>
document.querySelectorAll('img').forEach(img => {
  img.addEventListener('click', e => {
    const lb = document.querySelector('.lightbox');
    lb.querySelector('img').src = img.src;
    lb.classList.add('active');
  });
});

document.querySelector('.lightbox').addEventListener('click', () => {
  document.querySelector('.lightbox').classList.remove('active');
});
</script>
